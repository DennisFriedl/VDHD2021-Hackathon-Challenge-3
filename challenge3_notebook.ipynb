{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd02d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DHd-Hackathon 2021 Challenge 3\n",
    "Von Kai Krüger und Dennis Friedl (Universität Paderborn)\n",
    "\n",
    "Der folgende Code trainiert ein Modell für Named-entity recognition in einem frühneuhochdeutschen Text. Dafür wird die Python-Bibliothek \"Spacy\" genutzt. Seit der Version 3 hat sich Spacy grundlegend geändert. Falls mit einer Spacy-Version unter 2 gearbeitet wird, kann der auskommentierte Code der dritten Zelle genutzt werden, um die Trainings-CSV in das von Spacy 2 erwartete Format zu bringen. Für Spacy 3 ist dieser Schritt nicht mehr notwendig."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin\n",
    "import pandas as pd\n",
    "#import re # nur für Spacy 2 notwenig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUR FÜR SPACY 2:\n",
    "# train = pd.read_csv('./Daten/train.csv')\n",
    "# point_locations = train.loc[train.isin(['.']).any(axis=1)].index.tolist()\n",
    "# list_of_sentences = []\n",
    "# davor = 0 \n",
    "# for p in point_locations:\n",
    "#     list_of_sentences.append(train.iloc[davor:p+1, :])\n",
    "#     davor = p+1\n",
    "# list_of_sentences\n",
    "# TRAIN_DATA = []\n",
    "# for df in list_of_sentences:\n",
    "#     tokens = df['Token'].tolist()\n",
    "#     tags = df['Tag'].tolist()\n",
    "#     satz = ' '.join(tokens)\n",
    "#     info = []\n",
    "#     for i in range(len(tokens)):\n",
    "#         if tags[i] != \"O\":\n",
    "#             for match in re.finditer(tokens[i], satz):\n",
    "#                 s = match.start()\n",
    "#                 e = match.end()\n",
    "#                 dic1 = {\"token\": tokens[i], \"tag\": tags[i], \"start\": s, \"end\": e}\n",
    "#                 if dic1 not in info:\n",
    "#                     info.append(dic1)\n",
    "#     tail = []\n",
    "#     for ent in info:\n",
    "#         tail.append((ent[\"start\"], ent[\"end\"], ent[\"tag\"]))\n",
    "#     sentence_with_info = (satz, {\"entities\": tail})\n",
    "#     if sentence_with_info[1][\"entities\"]:\n",
    "#         TRAIN_DATA.append(sentence_with_info)"
   ]
  },
  {
   "source": [
    "## Bringt die Trainings-CSV in das von Spacy 3 erwartete Format:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\") # Deutschsprachiges Modell\n",
    "docbin = DocBin()\n",
    "df = pd.read_csv(\"./Daten/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['Token'].tolist() # Alle Tokens in eine Liste\n",
    "spaces = [False if w in '.,;:!?' else True for w in words] # Wann sollen Leerzeichen eingefügt werden und wann nicht?\n",
    "ents = df['Tag'].tolist() # Alle Tags in eine Liste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\n",
    "docbin.add(doc)\n",
    "docbin.to_disk(\"./train.spacy\")\n",
    "# Output ist eine Spacy-Trainingsdatei"
   ]
  },
  {
   "source": [
    "## Erstellen einer Config-Datei:\n",
    "\n",
    "Die vielen Stellschrauben eines Modelltrainings müssen mit Spacy 3 nicht mehr in Python-Code modifiziert werden, sondern laufen über eine Konfigurationsdatei. Um diese Datei zu initialisieren, können auf der Webseite von Spacy https://spacy.io/usage/training zunächst einfache Parameter wie die Sprache und die Komponenten (in unserem Fall NER) eingestellt werden, bevor die Datei dann heruntergeladen werden kann. Diese Datei muss dann noch weiter mit dem Konsolenbefehl ```python -m spacy init fill-config base_config.cfg config.cfg``` abschließend initialisiert werden.\n",
    "\n",
    "Die Konfigurationsdatei kann nun in einem Editor ganz normal geöffnet werden. Hier lohnt es sich mit Sicherheit, etwas mit den Werten und Einstellungen herumzuspielen, um die besten Parameter für frühneuzeitliche Texte herauszufinden. \n",
    "\n",
    "Auf was auf jeden Fall geachtet werden muss, ist dass in den Einstellungen für das Trainingsset die Parameter für \"max_length\" und \"limit\" auf jeweils \"0\", also unendlich, gesetzt sind. Wir hatten deswegen eine Fehlermeldung, die wir lange Zeit nicht zuordnen konnten.\n",
    "\n",
    " \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Trainieren des Modells:\n",
    "\n",
    "Für das Training an sich braucht man tatsächlich kein Python mehr, denn die Bedienung von Spacy funktioniert seit Spacy 3 ganz einfach über die Konsole. Dafür gibt man, nachdem die Scpay-Trainingsdatei und die Config erstellt wurden, folgendes in die Konsole: ```python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy```\n",
    "\n",
    "In diesem Fall läuft das Training über die CPU. Falls ein Training über GPU erwünscht ist, kann außerdem der Befehl ```--gpu-id 0``` angefügt werden. In der Konsole kann abgelesen werden, wie sich Daten wie Recall, Precision etc. im Laufe des Trainings ändern. Im Ordner \"Output\" finden sich jetzt sowohl das letzte als auch das beste Modell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Anwenden des Modells an der test.csv:\n",
    "\n",
    "Das Modell kann nun an der bereitgestellten test.csv getestet werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Taggen des Textes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"./output/model-best\") # lädt das beste Modell\n",
    "df = pd.read_csv(\"./Daten/test.csv\") # lädt die test.csv\n",
    "\n",
    "doc = ' '.join(df[\"Token\"].tolist()) # fügt die Tokens zu einem zusammenhängenden Text zusammen\n",
    "doc = nlp1(doc) # Anwendung des Modells\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # Anzeige des getaggten Textes"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Übersetzen in das gewünschte Format (mit I- und B-Präfix):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []    \n",
    "ent_list_dirty = []\n",
    "for t in doc: # Speichere Tokens und Tags in Liste\n",
    "    tokens.append(t.text)\n",
    "    ent_list_dirty.append(t.ent_type_)\n",
    "\n",
    "ent_list = []\n",
    "for i in range(len(ent_list_dirty)): # Anpassung der Tags als \"O\" oder mit dem I- oder B-Präfix\n",
    "    if ent_list_dirty[i] == \"\":\n",
    "       ent_list.append(\"O\")\n",
    "    else:\n",
    "        if i > 1 and ent_list_dirty[i-1] == ent_list_dirty[i]:\n",
    "            ent_list.append(\"I-\" + ent_list_dirty[i])\n",
    "        else:\n",
    "            ent_list.append(\"B-\" + ent_list_dirty[i])"
   ]
  },
  {
   "source": [
    "### Speichern des getaggten Textes als CSV:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(list(zip(tokens, ent_list)),\n",
    "columns = ['Tokens', 'Tag'])\n",
    "\n",
    "output_df.to_csv(\"test_output.csv\")"
   ]
  }
 ]
}